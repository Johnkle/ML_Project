{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle event 推荐比赛 — 特征工程\n",
    "By Johnkle\n",
    "* 数据清洗与预处理\n",
    "* 构建特征(包括协同过滤推荐度等复杂特征)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.引入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import datetime\n",
    "import hashlib\n",
    "import locale\n",
    "import numpy as np\n",
    "import pycountry\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as ss\n",
    "import scipy.spatial.distance as ssd\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据清洗类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner():\n",
    "    \"\"\"\n",
    "    Common utilities for converting strings to equivalent numbers\n",
    "    or number buckets.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 载入 locales\n",
    "        self.localeIdMap = defaultdict(int)\n",
    "        for i, l in enumerate(locale.locale_alias.keys()):\n",
    "            self.localeIdMap[l] = i + 1\n",
    "        # 载入 countries\n",
    "        self.countryIdMap = defaultdict(int)\n",
    "        ctryIdx = defaultdict(int)\n",
    "        for i, c in enumerate(pycountry.countries):\n",
    "            self.countryIdMap[c.name.lower()] = i + 1\n",
    "            if c.name.lower() == \"usa\":\n",
    "                ctryIdx[\"US\"] = i\n",
    "            if c.name.lower() == \"canada\":\n",
    "                ctryIdx[\"CA\"] = i\n",
    "        for cc in ctryIdx.keys():\n",
    "            for s in pycountry.subdivisions.get(country_code=cc):\n",
    "                self.countryIdMap[s.name.lower()] = ctryIdx[cc] + 1\n",
    "            # 载入 gender id 字典\n",
    "            self.genderIdMap = defaultdict(int, {\"male\":1, \"female\":2})\n",
    "\n",
    "    def getLocaleId(self, locstr):\n",
    "        return self.localeIdMap[locstr.lower()]\n",
    "\n",
    "    def getGenderId(self, genderStr):\n",
    "        return self.genderIdMap[genderStr]\n",
    "\n",
    "    def getJoinedYearMonth(self, dateString):\n",
    "        dttm = datetime.datetime.strptime(dateString, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        return \"\".join([str(dttm.year), str(dttm.month)])\n",
    "\n",
    "    def getCountryId(self, location):\n",
    "        if (isinstance(location, str)\n",
    "            and len(location.strip()) > 0\n",
    "            and location.rfind(\"  \") > -1):\n",
    "            return self.countryIdMap[location[location.rindex(\"  \") + 2:].lower()]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def getBirthYearInt(self, birthYear):\n",
    "        try:\n",
    "            return 0 if birthYear == \"None\" else int(birthYear)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def getTimezoneInt(self, timezone):\n",
    "        try:\n",
    "            return int(timezone)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def getFeatureHash(self, value):\n",
    "        if len(value.strip()) == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return int(hashlib.sha224(value).hexdigest()[0:4], 16)\n",
    "\n",
    "    def getFloatValue(self, value):\n",
    "        if len(value.strip()) == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.处理user和event关联数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgramEntities():\n",
    "  \"\"\"\n",
    "  我们只关心train和test中出现的user和event，因此重点处理这部分关联数据\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    # 统计训练集中有多少独立的用户的events\n",
    "    dpath = \"./data/\"\n",
    "    uniqueUsers = set()\n",
    "    uniqueEvents = set()\n",
    "    eventsForUser = defaultdict(set)\n",
    "    usersForEvent = defaultdict(set)\n",
    "    for filename in [dpath+\"train.csv\", dpath+\"test.csv\"]:\n",
    "        f = open(filename, 'r')\n",
    "        f.readline().strip().split(\",\")\n",
    "        for line in f:\n",
    "            cols = line.strip().split(\",\")\n",
    "            uniqueUsers.add(cols[0])\n",
    "            uniqueEvents.add(cols[1])\n",
    "            eventsForUser[cols[0]].add(cols[1])\n",
    "            usersForEvent[cols[1]].add(cols[0])\n",
    "        f.close()\n",
    "    pickle.dump(uniqueUsers, open(\"./package/PE_uniqueUsers.pkl\", 'wb'))\n",
    "    pickle.dump(uniqueEvents, open(\"./package/PE_uniqueEvents.pkl\", 'wb'))\n",
    "    pickle.dump(eventsForUser, open(\"./package/PE_eventsForUser.pkl\", 'wb'))\n",
    "    pickle.dump(usersForEvent, open(\"./package/PE_usersForEvent.pkl\", 'wb'))\n",
    "    self.userEventScores = ss.dok_matrix((len(uniqueUsers), len(uniqueEvents)))\n",
    "    self.userIndex = dict()\n",
    "    self.eventIndex = dict()\n",
    "    for i, u in enumerate(uniqueUsers):\n",
    "        self.userIndex[u] = i\n",
    "    for i, e in enumerate(uniqueEvents):\n",
    "        self.eventIndex[e] = i\n",
    "    ftrain = open(dpath+\"train.csv\", 'r')\n",
    "    ftrain.readline()\n",
    "    for line in ftrain:\n",
    "        cols = line.strip().split(\",\")\n",
    "        i = self.userIndex[cols[0]]\n",
    "        j = self.eventIndex[cols[1]]\n",
    "        self.userEventScores[i, j] = int(cols[4]) - int(cols[5])\n",
    "    ftrain.close()\n",
    "    pickle.dump(self.userIndex, open(\"./package/PE_userIndex.pkl\", 'wb'))\n",
    "    pickle.dump(self.eventIndex, open(\"./package/PE_eventIndex.pkl\", 'wb'))\n",
    "    sio.mmwrite(\"./package/PE_userEventScores\", self.userEventScores)\n",
    "    # 为了防止不必要的计算，我们找出来所有关联的用户 或者 关联的event\n",
    "    # 所谓的关联用户，指的是至少在同一个event上有行为的用户pair\n",
    "    # 关联的event指的是至少同一个user有行为的event pair\n",
    "    self.uniqueUserPairs = set()\n",
    "    self.uniqueEventPairs = set()\n",
    "    for event in uniqueEvents:\n",
    "        users = usersForEvent[event]\n",
    "        if len(users) > 2:\n",
    "            self.uniqueUserPairs.update(itertools.combinations(users, 2))\n",
    "    for user in uniqueUsers:\n",
    "        events = eventsForUser[user]\n",
    "        if len(events) > 2:\n",
    "            self.uniqueEventPairs.update(itertools.combinations(events, 2))\n",
    "    pickle.dump(self.uniqueUserPairs, open(\"./package/PE_uniqueUserPairs.pkl\", 'wb'))\n",
    "    pickle.dump(self.uniqueEventPairs, open(\"./package/PE_uniqueEventPairs.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pe = ProgramEntities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.用户与用户相似度矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Users():\n",
    "    \"\"\"\n",
    "    构建 user/user 相似度矩阵\n",
    "    \"\"\"\n",
    "    def __init__(self,sim=ssd.correlation):\n",
    "        userIndex = pickle.load(open(\"./package/PE_userIndex.pkl\", 'rb'))\n",
    "        uniqueUserPairs = pickle.load(open(\"./package/PE_uniqueUserPairs.pkl\", 'rb'))\n",
    "        cleaner = DataCleaner()\n",
    "        nusers = len(userIndex.keys())\n",
    "        fin = open(\"./data/users.csv\", 'r')\n",
    "        colnames = fin.readline().strip().split(\",\")\n",
    "        self.userMatrix = ss.dok_matrix((nusers, len(colnames) - 1))\n",
    "        for line in fin:\n",
    "            cols = line.strip().split(\",\")\n",
    "          # 只考虑train.csv中出现的用户\n",
    "            if cols[0] in userIndex:\n",
    "                i = userIndex[cols[0]]\n",
    "                self.userMatrix[i, 0] = cleaner.getLocaleId(cols[1])\n",
    "                self.userMatrix[i, 1] = cleaner.getBirthYearInt(cols[2])\n",
    "                self.userMatrix[i, 2] = cleaner.getGenderId(cols[3])\n",
    "                self.userMatrix[i, 3] = cleaner.getJoinedYearMonth(cols[4])\n",
    "                self.userMatrix[i, 4] = cleaner.getCountryId(cols[5])\n",
    "                self.userMatrix[i, 5] = cleaner.getTimezoneInt(cols[6])\n",
    "        fin.close()\n",
    "        # 归一化用户矩阵\n",
    "        self.userMatrix = normalize(self.userMatrix, norm=\"l1\", axis=0, copy=False)\n",
    "        sio.mmwrite(\"./package/US_userMatrix\", self.userMatrix)\n",
    "        # 计算用户相似度矩阵，之后会用到\n",
    "        self.userSimMatrix = ss.dok_matrix((nusers, nusers))\n",
    "        for i in range(0, nusers):\n",
    "            self.userSimMatrix[i, i] = 1.0\n",
    "        for u1, u2 in uniqueUserPairs:\n",
    "            i = userIndex[u1]\n",
    "            j = userIndex[u2]\n",
    "            if (i,j) not in self.userSimMatrix:\n",
    "                usim = sim(self.userMatrix.getrow(i).todense(),\\\n",
    "                           self.userMatrix.getrow(j).todense())\n",
    "                self.userSimMatrix[i, j] = usim\n",
    "                self.userSimMatrix[j, i] = usim\n",
    "        sio.mmwrite(\"./package/US_userSimMatrix\", self.userSimMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Users at 0x244872f5828>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.用户社交关系挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserFriends():\n",
    "    \"\"\"\n",
    "    找出某用户的那些朋友，想法非常简单\n",
    "    1)如果你有更多的朋友，可能你性格外向，更容易参加各种活动\n",
    "    2)如果你朋友会参加某个活动，可能你也会跟随去参加一下\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        userIndex = pickle.load(open(\"./package/PE_userIndex.pkl\", 'rb'))\n",
    "        eventsForUser = pickle.load(open(\"./package/PE_eventsForUser.pkl\", 'rb'))\n",
    "        userEventScores = sio.mmread(\"./package/PE_userEventScores\")\n",
    "        nusers = len(userIndex.keys())\n",
    "        self.numFriends = np.zeros((nusers))\n",
    "        self.userFriends = ss.dok_matrix((nusers, nusers))\n",
    "        fin = open(\"./data/user_friends.csv\", 'r')\n",
    "        fin.readline()                # skip header\n",
    "        ln = 0\n",
    "        for line in fin:\n",
    "            if ln % 200 == 0:\n",
    "                print (\"Loading line: \", ln)\n",
    "            cols = line.strip().split(\",\")\n",
    "            user = cols[0]\n",
    "            if user in userIndex:\n",
    "                friends = cols[1].split(\" \")\n",
    "                i = userIndex[user]\n",
    "                self.numFriends[i] = len(friends)\n",
    "                for friend in friends:\n",
    "                    if friend in userIndex:\n",
    "                        j = userIndex[friend]\n",
    "                        #eventsForUser不是变量，为什么赋值，导致userFriends无值\n",
    "                        eventsForUser = userEventScores.getrow(j).todense()\n",
    "                        score = eventsForUser.sum() / np.shape(eventsForUser)[1]\n",
    "                        self.userFriends[i, j] += score\n",
    "                        self.userFriends[j, i] += score\n",
    "            ln += 1\n",
    "        fin.close()\n",
    "        # 归一化数组\n",
    "        sumNumFriends = self.numFriends.sum(axis=0)\n",
    "        self.numFriends = self.numFriends / sumNumFriends\n",
    "        sio.mmwrite(\"./package/UF_numFriends\", np.matrix(self.numFriends))\n",
    "        self.userFriends = normalize(self.userFriends, norm=\"l1\", axis=0, copy=False)\n",
    "        sio.mmwrite(\"./package/UF_userFriends\", self.userFriends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading line:  0\n",
      "Loading line:  200\n",
      "Loading line:  400\n",
      "Loading line:  600\n",
      "Loading line:  800\n",
      "Loading line:  1000\n",
      "Loading line:  1200\n",
      "Loading line:  1400\n",
      "Loading line:  1600\n",
      "Loading line:  1800\n",
      "Loading line:  2000\n",
      "Loading line:  2200\n",
      "Loading line:  2400\n",
      "Loading line:  2600\n",
      "Loading line:  2800\n",
      "Loading line:  3000\n",
      "Loading line:  3200\n",
      "Loading line:  3400\n",
      "Loading line:  3600\n",
      "Loading line:  3800\n",
      "Loading line:  4000\n",
      "Loading line:  4200\n",
      "Loading line:  4400\n",
      "Loading line:  4600\n",
      "Loading line:  4800\n",
      "Loading line:  5000\n",
      "Loading line:  5200\n",
      "Loading line:  5400\n",
      "Loading line:  5600\n",
      "Loading line:  5800\n",
      "Loading line:  6000\n",
      "Loading line:  6200\n",
      "Loading line:  6400\n",
      "Loading line:  6600\n",
      "Loading line:  6800\n",
      "Loading line:  7000\n",
      "Loading line:  7200\n",
      "Loading line:  7400\n",
      "Loading line:  7600\n",
      "Loading line:  7800\n",
      "Loading line:  8000\n",
      "Loading line:  8200\n",
      "Loading line:  8400\n",
      "Loading line:  8600\n",
      "Loading line:  8800\n",
      "Loading line:  9000\n",
      "Loading line:  9200\n",
      "Loading line:  9400\n",
      "Loading line:  9600\n",
      "Loading line:  9800\n",
      "Loading line:  10000\n",
      "Loading line:  10200\n",
      "Loading line:  10400\n",
      "Loading line:  10600\n",
      "Loading line:  10800\n",
      "Loading line:  11000\n",
      "Loading line:  11200\n",
      "Loading line:  11400\n",
      "Loading line:  11600\n",
      "Loading line:  11800\n",
      "Loading line:  12000\n",
      "Loading line:  12200\n",
      "Loading line:  12400\n",
      "Loading line:  12600\n",
      "Loading line:  12800\n",
      "Loading line:  13000\n",
      "Loading line:  13200\n",
      "Loading line:  13400\n",
      "Loading line:  13600\n",
      "Loading line:  13800\n",
      "Loading line:  14000\n",
      "Loading line:  14200\n",
      "Loading line:  14400\n",
      "Loading line:  14600\n",
      "Loading line:  14800\n",
      "Loading line:  15000\n",
      "Loading line:  15200\n",
      "Loading line:  15400\n",
      "Loading line:  15600\n",
      "Loading line:  15800\n",
      "Loading line:  16000\n",
      "Loading line:  16200\n",
      "Loading line:  16400\n",
      "Loading line:  16600\n",
      "Loading line:  16800\n",
      "Loading line:  17000\n",
      "Loading line:  17200\n",
      "Loading line:  17400\n",
      "Loading line:  17600\n",
      "Loading line:  17800\n",
      "Loading line:  18000\n",
      "Loading line:  18200\n",
      "Loading line:  18400\n",
      "Loading line:  18600\n",
      "Loading line:  18800\n",
      "Loading line:  19000\n",
      "Loading line:  19200\n",
      "Loading line:  19400\n",
      "Loading line:  19600\n",
      "Loading line:  19800\n",
      "Loading line:  20000\n",
      "Loading line:  20200\n",
      "Loading line:  20400\n",
      "Loading line:  20600\n",
      "Loading line:  20800\n",
      "Loading line:  21000\n",
      "Loading line:  21200\n",
      "Loading line:  21400\n",
      "Loading line:  21600\n",
      "Loading line:  21800\n",
      "Loading line:  22000\n",
      "Loading line:  22200\n",
      "Loading line:  22400\n",
      "Loading line:  22600\n",
      "Loading line:  22800\n",
      "Loading line:  23000\n",
      "Loading line:  23200\n",
      "Loading line:  23400\n",
      "Loading line:  23600\n",
      "Loading line:  23800\n",
      "Loading line:  24000\n",
      "Loading line:  24200\n",
      "Loading line:  24400\n",
      "Loading line:  24600\n",
      "Loading line:  24800\n",
      "Loading line:  25000\n",
      "Loading line:  25200\n",
      "Loading line:  25400\n",
      "Loading line:  25600\n",
      "Loading line:  25800\n",
      "Loading line:  26000\n",
      "Loading line:  26200\n",
      "Loading line:  26400\n",
      "Loading line:  26600\n",
      "Loading line:  26800\n",
      "Loading line:  27000\n",
      "Loading line:  27200\n",
      "Loading line:  27400\n",
      "Loading line:  27600\n",
      "Loading line:  27800\n",
      "Loading line:  28000\n",
      "Loading line:  28200\n",
      "Loading line:  28400\n",
      "Loading line:  28600\n",
      "Loading line:  28800\n",
      "Loading line:  29000\n",
      "Loading line:  29200\n",
      "Loading line:  29400\n",
      "Loading line:  29600\n",
      "Loading line:  29800\n",
      "Loading line:  30000\n",
      "Loading line:  30200\n",
      "Loading line:  30400\n",
      "Loading line:  30600\n",
      "Loading line:  30800\n",
      "Loading line:  31000\n",
      "Loading line:  31200\n",
      "Loading line:  31400\n",
      "Loading line:  31600\n",
      "Loading line:  31800\n",
      "Loading line:  32000\n",
      "Loading line:  32200\n",
      "Loading line:  32400\n",
      "Loading line:  32600\n",
      "Loading line:  32800\n",
      "Loading line:  33000\n",
      "Loading line:  33200\n",
      "Loading line:  33400\n",
      "Loading line:  33600\n",
      "Loading line:  33800\n",
      "Loading line:  34000\n",
      "Loading line:  34200\n",
      "Loading line:  34400\n",
      "Loading line:  34600\n",
      "Loading line:  34800\n",
      "Loading line:  35000\n",
      "Loading line:  35200\n",
      "Loading line:  35400\n",
      "Loading line:  35600\n",
      "Loading line:  35800\n",
      "Loading line:  36000\n",
      "Loading line:  36200\n",
      "Loading line:  36400\n",
      "Loading line:  36600\n",
      "Loading line:  36800\n",
      "Loading line:  37000\n",
      "Loading line:  37200\n",
      "Loading line:  37400\n",
      "Loading line:  37600\n",
      "Loading line:  37800\n",
      "Loading line:  38000\n",
      "Loading line:  38200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.UserFriends at 0x244859630f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UserFriends()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.构造event和event相似度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Events():\n",
    "    \"\"\"\n",
    "    构建event-event相似度，注意这里有2种相似度：\n",
    "    1）由用户-event行为，类似协同过滤算出的相似度\n",
    "    2）由event本身的内容(event信息)计算出的event-event相似度\n",
    "    \"\"\"\n",
    "    def __init__(self, psim=ssd.correlation, csim=ssd.cosine):\n",
    "        eventIndex = pickle.load(open(\"./package/PE_eventIndex.pkl\", 'rb'))\n",
    "        uniqueEventPairs = pickle.load(open(\"./package/PE_uniqueEventPairs.pkl\", 'rb'))\n",
    "        cleaner = DataCleaner()\n",
    "        fin = open(\"./data/events.csv\", 'r')\n",
    "        fin.readline() # skip header\n",
    "        nevents = len(eventIndex.keys())\n",
    "        self.eventPropMatrix = ss.dok_matrix((nevents, 7))\n",
    "        self.eventContMatrix = ss.dok_matrix((nevents, 100))\n",
    "        ln = 0\n",
    "        for line in fin.readlines():\n",
    "    #      if ln > 10:\n",
    "    #        break\n",
    "            cols = line.strip().split(\",\")\n",
    "            eventId = cols[0]\n",
    "            if eventId in eventIndex:\n",
    "                i = eventIndex[eventId]\n",
    "                self.eventPropMatrix[i, 0] = cleaner.getJoinedYearMonth(cols[2]) # start_time\n",
    "                #Unicode-objects must be encoded before hashing\n",
    "                self.eventPropMatrix[i, 1] = cleaner.getFeatureHash(cols[3].encode('utf-8')) # city\n",
    "                self.eventPropMatrix[i, 2] = cleaner.getFeatureHash(cols[4].encode('utf-8')) # state\n",
    "                self.eventPropMatrix[i, 3] = cleaner.getFeatureHash(cols[5].encode('utf-8')) # zip\n",
    "                self.eventPropMatrix[i, 4] = cleaner.getFeatureHash(cols[6].encode('utf-8')) # country\n",
    "                self.eventPropMatrix[i, 5] = cleaner.getFloatValue(cols[7]) # lat\n",
    "                self.eventPropMatrix[i, 6] = cleaner.getFloatValue(cols[8]) # lon\n",
    "                for j in range(9, 109):\n",
    "                    self.eventContMatrix[i, j-9] = cols[j]\n",
    "            ln += 1\n",
    "        fin.close()\n",
    "        self.eventPropMatrix = normalize(self.eventPropMatrix,\n",
    "            norm=\"l1\", axis=0, copy=False)\n",
    "        sio.mmwrite(\"./package/EV_eventPropMatrix\", self.eventPropMatrix)\n",
    "        self.eventContMatrix = normalize(self.eventContMatrix,\n",
    "            norm=\"l1\", axis=0, copy=False)\n",
    "        sio.mmwrite(\"./package/EV_eventContMatrix\", self.eventContMatrix)\n",
    "        # calculate similarity between event pairs based on the two matrices    \n",
    "        self.eventPropSim = ss.dok_matrix((nevents, nevents))\n",
    "        self.eventContSim = ss.dok_matrix((nevents, nevents))\n",
    "        for e1, e2 in uniqueEventPairs:\n",
    "            i = eventIndex[e1]\n",
    "            j = eventIndex[e2]\n",
    "            if (i,j) not in self.eventPropSim:\n",
    "                epsim = psim(self.eventPropMatrix.getrow(i).todense(),\n",
    "                  self.eventPropMatrix.getrow(j).todense())\n",
    "                self.eventPropSim[i, j] = epsim\n",
    "                self.eventPropSim[j, i] = epsim\n",
    "            if (i,j) not in self.eventContSim:\n",
    "                ecsim = csim(self.eventContMatrix.getrow(i).todense(),\\\n",
    "                             self.eventContMatrix.getrow(j).todense())\n",
    "                self.eventContSim[i, j] = epsim\n",
    "                self.eventContSim[j, i] = epsim\n",
    "        sio.mmwrite(\"./package/EV_eventPropSim\", self.eventPropSim)\n",
    "        sio.mmwrite(\"./package/EV_eventContSim\", self.eventContSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\lib\\site-packages\\scipy\\spatial\\distance.py:702: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Events at 0x24484e786d8>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.活跃度/event热度 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventAttendees():\n",
    "    \"\"\"\n",
    "    统计某个活动，参加和不参加的人数，从而为活动活跃度做准备\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        eventIndex = pickle.load(open(\"./package/PE_eventIndex.pkl\", 'rb'))\n",
    "        nevents = len(eventIndex.keys())\n",
    "        self.eventPopularity = ss.dok_matrix((nevents, 1))\n",
    "        f = open(\"./data/event_attendees.csv\", 'rb')\n",
    "        f.readline() # skip header\n",
    "        for line in f:\n",
    "            cols = line.strip().split(\",\")\n",
    "            eventId = cols[0]\n",
    "            if eventId in eventIndex:\n",
    "                i = eventIndex[eventId]\n",
    "                self.eventPopularity[i, 0] = \\\n",
    "                len(cols[1].split(\" \")) - len(cols[4].split(\" \"))\n",
    "        f.close()\n",
    "        self.eventPopularity = normalize(self.eventPopularity, norm=\"l1\",\n",
    "                                         axis=0, copy=False)\n",
    "        sio.mmwrite(\"./package/EA_eventPopularity\", self.eventPopularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EventAttendees at 0x244850c8240>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EventAttendees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFM隐语义推荐度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfm_train(train_path, F, alpha, beta, step):\n",
    "    \"\"\"\n",
    "    train LFM model,get latent factor user_vec and event_vec\n",
    "    Args:\n",
    "        train_data: train_data for lfm\n",
    "        F: user vector len, event vector len\n",
    "        alpha:regularization factor\n",
    "        beta: learning rate\n",
    "        step: iteration number\n",
    "    Return:\n",
    "        dict: key eventid, value:np.ndarray\n",
    "        dict: key userid, value:np.ndarray\n",
    "    \"\"\"\n",
    "    user_vec = {}\n",
    "    event_vec = {}\n",
    "    count = 0\n",
    "    for step in range(step):\n",
    "        fin = open(train_path,\"r+\")\n",
    "        start = 0\n",
    "        #每次取一行，随机梯度下降？\n",
    "        for line in fin:\n",
    "            if start == 0:\n",
    "                start += 1\n",
    "                continue\n",
    "            cols = line.strip().split(\",\")\n",
    "            userid,eventid,label = cols[0],cols[1],cols[-2]\n",
    "            if userid not in user_vec:\n",
    "                user_vec[userid] = np.random.randn(F)\n",
    "            if eventid not in event_vec:\n",
    "                event_vec[eventid] = np.random.randn(F)\n",
    "            #label是str，需转换为int\n",
    "            delta = int(label)-lfm_score(user_vec[userid],event_vec[eventid])\n",
    "            for i in range(F):\n",
    "                user_vec[userid][i] += beta*(delta*event_vec[eventid][i]\\\n",
    "                                            -alpha*user_vec[userid][i])\n",
    "                event_vec[eventid][i] += beta*(delta*user_vec[userid][i]\\\n",
    "                                            -alpha*event_vec[eventid][i])\n",
    "            count += 1\n",
    "            #第1轮不更新学习率\n",
    "            if step == 0:\n",
    "                continue\n",
    "            #每2000个样本更新一次学习率\n",
    "            if count%2000==0:\n",
    "                beta *= 0.95\n",
    "            if count%5000==0:\n",
    "                print(\"step %d,count %d,learning rate %g:\"%(step, count, beta))\n",
    "    pickle.dump(user_vec,open(\"./package/LFM_user_vec.pkl\",\"wb\"))\n",
    "    pickle.dump(event_vec,open(\"./package/LFM_event_vec.pkl\",\"wb\"))\n",
    "\n",
    "def lfm_score(user_vector,event_vector):\n",
    "    \"\"\"\n",
    "    user_vector and event_vector distance\n",
    "    Args:\n",
    "        user_vector: lfm model produce user vector\n",
    "        event_vector: lfm model produce event vector\n",
    "    Return:\n",
    "         lfm recommend score\n",
    "    \"\"\"\n",
    "    score = np.dot(user_vector, event_vector)/\\\n",
    "                (np.linalg.norm(user_vector)*np.linalg.norm(event_vector))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1,count 20000,learning rate 0.0857375:\n",
      "step 1,count 25000,learning rate 0.0773781:\n",
      "step 1,count 30000,learning rate 0.066342:\n",
      "step 2,count 35000,learning rate 0.0598737:\n",
      "step 2,count 40000,learning rate 0.0513342:\n",
      "step 2,count 45000,learning rate 0.0463291:\n",
      "step 3,count 50000,learning rate 0.0397214:\n",
      "step 3,count 55000,learning rate 0.0358486:\n",
      "step 3,count 60000,learning rate 0.0307357:\n",
      "step 4,count 65000,learning rate 0.027739:\n",
      "step 4,count 70000,learning rate 0.0237827:\n",
      "step 4,count 75000,learning rate 0.0214639:\n",
      "step 5,count 80000,learning rate 0.0184026:\n",
      "step 5,count 85000,learning rate 0.0166083:\n",
      "step 5,count 90000,learning rate 0.0142396:\n",
      "step 6,count 95000,learning rate 0.0128512:\n",
      "step 6,count 100000,learning rate 0.0110183:\n",
      "step 6,count 105000,learning rate 0.00994403:\n",
      "step 7,count 110000,learning rate 0.00852576:\n",
      "step 7,count 115000,learning rate 0.0076945:\n",
      "step 7,count 120000,learning rate 0.00659707:\n",
      "step 8,count 125000,learning rate 0.00595386:\n",
      "step 8,count 130000,learning rate 0.00510469:\n",
      "step 8,count 135000,learning rate 0.00460698:\n",
      "step 9,count 140000,learning rate 0.00394991:\n",
      "step 9,count 145000,learning rate 0.00356479:\n",
      "step 9,count 150000,learning rate 0.00305636:\n",
      "step 10,count 155000,learning rate 0.00275837:\n",
      "step 10,count 160000,learning rate 0.00236496:\n",
      "step 10,count 165000,learning rate 0.00213437:\n",
      "step 11,count 170000,learning rate 0.00182996:\n",
      "step 11,count 175000,learning rate 0.00165154:\n",
      "step 11,count 180000,learning rate 0.00141599:\n",
      "step 12,count 185000,learning rate 0.00127793:\n",
      "step 12,count 190000,learning rate 0.00109566:\n",
      "step 12,count 195000,learning rate 0.000988836:\n",
      "step 12,count 200000,learning rate 0.000847804:\n",
      "step 13,count 205000,learning rate 0.000765143:\n",
      "step 13,count 210000,learning rate 0.000656014:\n",
      "step 13,count 215000,learning rate 0.000592053:\n",
      "step 14,count 220000,learning rate 0.000507611:\n",
      "step 14,count 225000,learning rate 0.000458119:\n",
      "step 14,count 230000,learning rate 0.00039278:\n",
      "step 15,count 235000,learning rate 0.000354484:\n",
      "step 15,count 240000,learning rate 0.000303926:\n",
      "step 15,count 245000,learning rate 0.000274293:\n",
      "step 16,count 250000,learning rate 0.000235172:\n",
      "step 16,count 255000,learning rate 0.000212243:\n",
      "step 16,count 260000,learning rate 0.000181972:\n",
      "step 17,count 265000,learning rate 0.000164229:\n",
      "step 17,count 270000,learning rate 0.000140806:\n",
      "step 17,count 275000,learning rate 0.000127078:\n",
      "step 18,count 280000,learning rate 0.000108953:\n",
      "step 18,count 285000,learning rate 9.83302e-05:\n",
      "step 18,count 290000,learning rate 8.43058e-05:\n",
      "step 19,count 295000,learning rate 7.6086e-05:\n",
      "step 19,count 300000,learning rate 6.52342e-05:\n",
      "step 19,count 305000,learning rate 5.88739e-05:\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lfm_train(\"./data/train.csv\", 40, 0.01, 0.1, 20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.串起所有的数据处理和准备流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第2步：计算用户相似度信息，并用矩阵形式存储...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "has_key not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mdata_prepare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-cfa857a14b05>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sim)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muserIndex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muserSimMatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m                 usim = sim(self.userMatrix.getrow(i).todense(),\\\n\u001b[0;32m     37\u001b[0m                            self.userMatrix.getrow(j).todense())\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: has_key not found"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def data_prepare():\n",
    "    \"\"\"\n",
    "    计算生成所有的数据，用矩阵或者其他形式存储方便后续提取特征和建模\n",
    "    \"\"\"\n",
    "    #print \"第1步：统计user和event相关信息...\"\n",
    "    #pe = ProgramEntities()\n",
    "    #print \"第1步完成...\\n\"\n",
    "    \n",
    "    print (\"第2步：计算用户相似度信息，并用矩阵形式存储...\")\n",
    "    Users()\n",
    "    print (\"第2步完成...\\n\")\n",
    "    \n",
    "    print (\"第3步：计算用户社交关系信息，并存储...\")\n",
    "    UserFriends()\n",
    "    print (\"第3步完成...\\n\")\n",
    "    \n",
    "    print (\"第4步：计算event相似度信息，并用矩阵形式存储...\")\n",
    "    Events()\n",
    "    print (\"第4步完成...\\n\")\n",
    "    \n",
    "    print (\"第5步：计算event热度信息...\")\n",
    "    EventAttendees()\n",
    "    print (\"第5步完成...\\n\")\n",
    "\n",
    "# 运行进行数据准备\n",
    "data_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.构建特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是构建特征部分\n",
    "from __future__ import division\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "class DataRewriter:\n",
    "    def __init__(self):\n",
    "        # 读入数据做初始化\n",
    "        self.userIndex = pickle.load(open(\"./package/PE_userIndex.pkl\", 'rb'))\n",
    "        self.eventIndex = pickle.load(open(\"./package/PE_eventIndex.pkl\", 'rb'))\n",
    "        self.userEventScores = sio.mmread(\"./package/PE_userEventScores\").todense()\n",
    "        self.userSimMatrix = sio.mmread(\"./package/US_userSimMatrix\").todense()\n",
    "        self.eventPropSim = sio.mmread(\"./package/EV_eventPropSim\").todense()\n",
    "        self.eventContSim = sio.mmread(\"./package/EV_eventContSim\").todense()\n",
    "        self.numFriends = sio.mmread(\"./package/UF_numFriends\")\n",
    "        self.userFriends = sio.mmread(\"./package/UF_userFriends\").todense()\n",
    "        self.eventPopularity = sio.mmread(\"./package/EA_eventPopularity\").todense()\n",
    "        self.user_vec = pickle.load(open(\"./package/LFM_user_vec.pkl\", 'rb'))\n",
    "        self.event_vec = pickle.load(open(\"./package/LFM_event_vec.pkl\", 'rb'))\n",
    "    \n",
    "    def userReco(self, userId, eventId):\n",
    "        \"\"\"\n",
    "        根据User-based协同过滤，得到event的推荐度\n",
    "        基本的伪代码思路如下：\n",
    "        for item i\n",
    "          for every other user v that has a preference for i\n",
    "            compute similarity s between u and v\n",
    "            incorporate v's preference for i weighted by s into running aversge\n",
    "        return top items ranked by weighted average\n",
    "        \"\"\"\n",
    "        i = self.userIndex[userId]\n",
    "        j = self.eventIndex[eventId]\n",
    "        vs = self.userEventScores[:, j]\n",
    "        sims = self.userSimMatrix[i, :]\n",
    "        prod = sims * vs\n",
    "        try:\n",
    "            return prod[0, 0] - self.userEventScores[i, j]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "\n",
    "    def eventReco(self, userId, eventId):\n",
    "        \"\"\"\n",
    "        根据基于物品的协同过滤，得到Event的推荐度\n",
    "        基本的伪代码思路如下：\n",
    "        for item i \n",
    "          for every item j tht u has a preference for\n",
    "            compute similarity s between i and j\n",
    "            add u's preference for j weighted by s to a running average\n",
    "        return top items, ranked by weighted average\n",
    "        \"\"\"\n",
    "        i = self.userIndex[userId]\n",
    "        j = self.eventIndex[eventId]\n",
    "        js = self.userEventScores[i, :]\n",
    "        psim = self.eventPropSim[:, j]\n",
    "        csim = self.eventContSim[:, j]\n",
    "        pprod = js * psim\n",
    "        cprod = js * csim\n",
    "        pscore = 0\n",
    "        cscore = 0\n",
    "        try:\n",
    "            pscore = pprod[0, 0] - self.userEventScores[i, j]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            cscore = cprod[0, 0] - self.userEventScores[i, j]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return pscore, cscore\n",
    "\n",
    "    def userPop(self, userId):\n",
    "        \"\"\"\n",
    "        基于用户的朋友个数来推断用户的社交程度\n",
    "        主要的考量是如果用户的朋友非常多，可能会更倾向于参加各种社交活动\n",
    "        \"\"\"\n",
    "        if userId in self.userIndex:\n",
    "            i = self.userIndex[userId]\n",
    "            try:\n",
    "                return self.numFriends[0, i]\n",
    "            except IndexError:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def friendInfluence(self, userId):\n",
    "        \"\"\"\n",
    "        朋友对用户的影响\n",
    "        主要考虑用户所有的朋友中，有多少是非常喜欢参加各种社交活动/event的\n",
    "        用户的朋友圈如果都积极参与各种event，可能会对当前用户有一定的影响\n",
    "        \"\"\"\n",
    "        nusers = np.shape(self.userFriends)[1]\n",
    "        i = self.userIndex[userId]\n",
    "        return (self.userFriends[i, :].sum(axis=0) / nusers)[0,0]\n",
    "\n",
    "    def eventPop(self, eventId):\n",
    "        \"\"\"\n",
    "        本活动本身的热度\n",
    "        主要是通过参与的人数来界定的\n",
    "        \"\"\"\n",
    "        i = self.eventIndex[eventId]\n",
    "        return self.eventPopularity[i, 0]\n",
    "    \n",
    "    def lfmReco(self,userId,eventId):\n",
    "        \"\"\"\n",
    "        基于LFM的推荐度\n",
    "        \"\"\"\n",
    "        #为什么不加这句会报错\n",
    "        lfm_res = 0.\n",
    "        if userId in self.user_vec and eventId in self.event_vec:\n",
    "            lfm_res = lfm_score(self.user_vec[userId],self.event_vec[eventId])\n",
    "            lfm_res = np.around(lfm_res,decimals=5)\n",
    "        return lfm_res\n",
    "\n",
    "    def rewriteData(self, start=1, train=True, header=True):\n",
    "        \"\"\"\n",
    "        把前面user-based协同过滤 和 item-based协同过滤，以及各种热度和影响度作为特征组合在一起\n",
    "        生成新的训练数据，用于分类器分类使用\n",
    "        \"\"\"\n",
    "        dpath = \"./data/\"\n",
    "        fn = \"train.csv\" if train else \"test.csv\"\n",
    "        fin = open(dpath+fn, 'r')\n",
    "        fout = open(dpath + \"data_\" + fn, 'w')\n",
    "        # write output header\n",
    "        if header:\n",
    "            ocolnames = [\"invited\", \"user_reco\", \"evt_p_reco\",\"evt_c_reco\",\\\n",
    "                         \"user_pop\", \"frnd_infl\", \"evt_pop\", \"lfm_reco\"]\n",
    "            if train:\n",
    "                ocolnames.append(\"interested\")\n",
    "                ocolnames.append(\"not_interested\")\n",
    "            fout.write(\",\".join(ocolnames) + \"\\n\")\n",
    "        ln = 0\n",
    "        for line in fin:\n",
    "            ln += 1\n",
    "            if ln < start:\n",
    "                continue\n",
    "            cols = line.strip().split(\",\")\n",
    "            userId = cols[0]\n",
    "            eventId = cols[1]\n",
    "            invited = cols[2]\n",
    "            if ln%500 == 0:\n",
    "                print (\"%s:%d (userId, eventId)=(%s, %s)\" % (fn, ln, userId, eventId))\n",
    "            user_reco = self.userReco(userId, eventId)\n",
    "            evt_p_reco, evt_c_reco = self.eventReco(userId, eventId)\n",
    "            user_pop = self.userPop(userId)\n",
    "            frnd_infl = self.friendInfluence(userId)\n",
    "            evt_pop = self.eventPop(eventId)\n",
    "            lfm_reco = self.lfmReco(userId,eventId)\n",
    "            ocols = [invited, user_reco, evt_p_reco,evt_c_reco,\\\n",
    "                     user_pop, frnd_infl, evt_pop, lfm_reco]\n",
    "            if train:\n",
    "                ocols.append(cols[4]) # interested\n",
    "                ocols.append(cols[5]) # not_interested\n",
    "            fout.write(\",\".join(map(lambda x: str(x), ocols)) + \"\\n\")\n",
    "        fin.close()\n",
    "        fout.close()\n",
    "\n",
    "    def rewriteTrainingSet(self):\n",
    "        self.rewriteData(True)\n",
    "\n",
    "    def rewriteTestSet(self):\n",
    "        self.rewriteData(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成训练数据...\n",
      "\n",
      "train.csv:500 (userId, eventId)=(123290209, 1887085024)\n",
      "train.csv:1000 (userId, eventId)=(272886293, 199858305)\n",
      "train.csv:1500 (userId, eventId)=(395305791, 1582270949)\n",
      "train.csv:2000 (userId, eventId)=(527523423, 3272728211)\n",
      "train.csv:2500 (userId, eventId)=(651258472, 792632006)\n",
      "train.csv:3000 (userId, eventId)=(811791433, 524756826)\n",
      "train.csv:3500 (userId, eventId)=(985547042, 1269035551)\n",
      "train.csv:4000 (userId, eventId)=(1107615001, 173949238)\n",
      "train.csv:4500 (userId, eventId)=(1236336671, 3849306291)\n",
      "train.csv:5000 (userId, eventId)=(1414301782, 2652356640)\n",
      "train.csv:5500 (userId, eventId)=(1595465532, 955398943)\n",
      "train.csv:6000 (userId, eventId)=(1747091728, 2131379889)\n",
      "train.csv:6500 (userId, eventId)=(1914182220, 955398943)\n",
      "train.csv:7000 (userId, eventId)=(2071842684, 1076364848)\n",
      "train.csv:7500 (userId, eventId)=(2217853337, 3051438735)\n",
      "train.csv:8000 (userId, eventId)=(2338481531, 2525447278)\n",
      "train.csv:8500 (userId, eventId)=(2489551967, 520657921)\n",
      "train.csv:9000 (userId, eventId)=(2650493630, 87962584)\n",
      "train.csv:9500 (userId, eventId)=(2791418962, 4223848259)\n",
      "train.csv:10000 (userId, eventId)=(2903662804, 2791462807)\n",
      "train.csv:10500 (userId, eventId)=(3036141956, 3929507420)\n",
      "train.csv:11000 (userId, eventId)=(3176074542, 3459485614)\n",
      "train.csv:11500 (userId, eventId)=(3285425249, 2271782630)\n",
      "train.csv:12000 (userId, eventId)=(3410667855, 1063772489)\n",
      "train.csv:12500 (userId, eventId)=(3531604778, 2584839423)\n",
      "train.csv:13000 (userId, eventId)=(3686871863, 53495098)\n",
      "train.csv:13500 (userId, eventId)=(3833637800, 2415873572)\n",
      "train.csv:14000 (userId, eventId)=(3944021305, 2096772901)\n",
      "train.csv:14500 (userId, eventId)=(4075466480, 3567240505)\n",
      "train.csv:15000 (userId, eventId)=(4197193550, 1628057176)\n",
      "生成预测数据...\n",
      "\n",
      "test.csv:500 (userId, eventId)=(182290053, 2529072432)\n",
      "test.csv:1000 (userId, eventId)=(433510318, 4244463632)\n",
      "test.csv:1500 (userId, eventId)=(632808865, 2845303452)\n",
      "test.csv:2000 (userId, eventId)=(813611885, 2036538169)\n",
      "test.csv:2500 (userId, eventId)=(1010701404, 303459881)\n",
      "test.csv:3000 (userId, eventId)=(1210932037, 2529072432)\n",
      "test.csv:3500 (userId, eventId)=(1452921099, 2705317682)\n",
      "test.csv:4000 (userId, eventId)=(1623287180, 1626678328)\n",
      "test.csv:4500 (userId, eventId)=(1855201342, 2603032829)\n",
      "test.csv:5000 (userId, eventId)=(2083900381, 2529072432)\n",
      "test.csv:5500 (userId, eventId)=(2318415276, 2509151803)\n",
      "test.csv:6000 (userId, eventId)=(2528161539, 4025975316)\n",
      "test.csv:6500 (userId, eventId)=(2749110768, 4244406355)\n",
      "test.csv:7000 (userId, eventId)=(2927772127, 1532377761)\n",
      "test.csv:7500 (userId, eventId)=(3199685636, 1776393554)\n",
      "test.csv:8000 (userId, eventId)=(3393388475, 680270887)\n",
      "test.csv:8500 (userId, eventId)=(3601169721, 154434302)\n",
      "test.csv:9000 (userId, eventId)=(3828963415, 3067222491)\n",
      "test.csv:9500 (userId, eventId)=(4018723397, 2522610844)\n",
      "test.csv:10000 (userId, eventId)=(4180064266, 2658555390)\n",
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dr = DataRewriter()\n",
    "print (\"生成训练数据...\\n\")\n",
    "dr.rewriteData(train=True, start=2, header=True)\n",
    "print (\"生成预测数据...\\n\")\n",
    "dr.rewriteData(train=False, start=2, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
